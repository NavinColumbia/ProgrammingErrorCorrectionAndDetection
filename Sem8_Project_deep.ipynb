{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqMQvBD3T-1e"
      },
      "outputs": [],
      "source": [
        "import torch as torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOB4d8QlxesM",
        "outputId": "10893653-e763-4b2d-8229-c75f2896827d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej-9QABgmfDt",
        "outputId": "d27ad3e4-d362-4037-8c79-28d9a707861b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T05I2u8nC0IK",
        "outputId": "c727d5e8-2774-4873-aa7f-51524df78675"
      },
      "outputs": [],
      "source": [
        "!tar xvzf /content/drive/MyDrive/final.tar.gz\n",
        "!tar xvzf /content/drive/MyDrive/vectors.tar.gz\n",
        "!cp  /content/drive/MyDrive/tokenizer.py tokenizer.py\n",
        "!cp /content/drive/MyDrive/models/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPo_oOdBWy5J"
      },
      "outputs": [],
      "source": [
        "from tokenizer import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi7DFKIGKUNy"
      },
      "outputs": [],
      "source": [
        "tokenlist = [\"\", *[f\"variable{x}\" for x in range(20)], *[f\"function{x}\" for x in range(30)],  \"continue\", \"unsigned\", \"default\", \"typedef\", \"define\", \"double\", \"extern\", \"signed\", \"sizeof\", \"static\", \"struct\", \"switch\", \"return\", \"break\", \"const\", \"float\", \"short\", \"union\", \"while\", \"auto\", \"case\", \"char\", \"else\", \"enum\", \"goto\", \"long\", \"main\", \"void\", \"for\", \"int\", \"do\", \"if\", \" \", \"!\", \"?\", \"_\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"’\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \":\", \";\", \"<\", \"=\", \">\", \"@\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"[\", \"\\\\\", \"]\", \"⌃\", \"‘\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"{\", \"|\", \"}\", \"∼\"]\n",
        "token_to_ix = {0:0}\n",
        "for i in enumerate(tokenlist):\n",
        "    token_to_ix[i[1]]=i[0]\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix.get(w,0) for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "def get_group(ix):\n",
        "    if ix>=token_to_ix[\"variable0\"] and ix<=token_to_ix[\"variable19\"]:\n",
        "        return \"variable\"\n",
        "    if ix>=token_to_ix[\"function0\"] and ix<=token_to_ix[\"function29\"]:\n",
        "        return \"function\"\n",
        "    if ix>=token_to_ix[\"continue\"] and ix<=token_to_ix[\"if\"]:\n",
        "        return \"keyword\"\n",
        "    if ix>=token_to_ix[\"A\"] and ix<=token_to_ix[\"Z\"]:\n",
        "        return \"alphabet\"\n",
        "    if ix>=token_to_ix[\"a\"] and ix<=token_to_ix[\"z\"]:\n",
        "        return \"alphabet\"\n",
        "    if ix>=token_to_ix[\"0\"] and ix<=token_to_ix[\"9\"]:\n",
        "        return \"numbers\"\n",
        "    return \"punctuation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "_G1JRKP_4H2U",
        "outputId": "a1d8e8af-bbba-4542-831b-029459615438"
      },
      "outputs": [],
      "source": [
        "class CDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file, limit=None):\n",
        "        with open(file, 'rb') as f:\n",
        "            if limit==None:\n",
        "                self.data = [(prepare_sequence(tk,token_to_ix), prepare_sequence(tg,token_to_ix)) for tk, tg in pickle.load(f)]\n",
        "            else:\n",
        "                self.data = [(prepare_sequence(tk,token_to_ix), prepare_sequence(tg,token_to_ix)) for tk, tg in pickle.load(f).head(limit)]\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idxs):\n",
        "        return self.data[idxs]\n",
        "\n",
        "dataset = CDataset('final.pckl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v-GraDQ2-qX"
      },
      "outputs": [],
      "source": [
        "class CCM(nn.Module):\n",
        "    def __init__(self, embedding_dim=32, hidden_dim=64, num_layers=4, vocab_size=178):\n",
        "        super(CCM, self).__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=0.5, batch_first=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        embeds = self.token_embeddings(x)\n",
        "        dropout = self.dropout(embeds.float())\n",
        "        lstm_out, _ = self.lstm(dropout.view(x.size(dim=0), x.size(dim=1), -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(x.size(dim=0), x.size(dim=1), -1))\n",
        "        tag_prob = F.softmax(tag_space, dim=2)\n",
        "        return tag_prob.transpose(1,2)\n",
        "model=CCM()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upUQf7B3rXWP"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"/content/deep_model_20230411_121014_fold_4_epoch_0\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi8DGQTo3r-L"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hphULWk76UzV"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb5gDZ6z61oK"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs.to(device))\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report?\n",
        "        if not torch.isnan(loss):\n",
        "            running_loss += loss.item()\n",
        "        else:\n",
        "            running_loss += 10\n",
        "        if i % 1000 == 999:\n",
        "\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "            torch.cuda.empty_cache()\n",
        "            mem = torch.cuda.mem_get_info(device)\n",
        "            print('  CUDA Memory: free {} / total {}'.format(mem[0], mem[1]))\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEvZKkpuJRcx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlZoviB_FzrT"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1RGFIBRJTR-"
      },
      "outputs": [],
      "source": [
        "!rm -rf runs/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gizato3S64zz"
      },
      "outputs": [],
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run?\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/error_detector_trainer_{}'.format(timestamp))\n",
        "run_number = 0\n",
        "\n",
        "batch_size=1\n",
        "folds=5\n",
        "epochs=1\n",
        "\n",
        "kfold=KFold(n_splits=folds, shuffle=True)\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for fold,(train_idx,test_idx) in enumerate(kfold.split(dataset)):\n",
        "    print('FOLD {}:'.format(fold+1))\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "    training_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=2, sampler=train_subsampler)\n",
        "    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=2, sampler=test_subsampler)\n",
        "\n",
        "    for epoch_number in range(epochs):\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        model.train(True)\n",
        "        avg_loss = train_one_epoch(fold*epochs+epoch_number, writer)\n",
        "\n",
        "        # We don't need gradients on to do reporting\n",
        "        model.train(False)\n",
        "\n",
        "        running_vloss = 0.0\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            voutputs = model(vinputs.to(device))\n",
        "            vloss = loss_fn(voutputs, vlabels.to(device))\n",
        "            if not(torch.isnan(vloss)):\n",
        "                running_vloss += vloss.item()\n",
        "\n",
        "        avg_vloss = running_vloss / (i + 1)\n",
        "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "        # Log the running loss averaged per batch\n",
        "        # for both training and validation\n",
        "        writer.add_scalars('Training vs. Validation Loss',\n",
        "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                        fold*epochs+epoch_number + 1)\n",
        "        writer.flush()\n",
        "\n",
        "        # Track best performance, and save the model's state\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_vloss\n",
        "        model_path = 'deep_model_{}_fold_{}_epoch_{}'.format(timestamp, fold, epoch_number)\n",
        "        g_model_path = '/content/drive/MyDrive/models/deep_model_{}_fold_{}_epoch_{}'.format(timestamp, fold, epoch_number)\n",
        "\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        torch.save(model.state_dict(), g_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEkeKRgEUfKY"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(c_str='''\n",
        "#include <stdio.h>\n",
        "int printf();\n",
        "int main(){\n",
        "  for (int i=0; i<200; i++){\n",
        "    printf(\"%d\\\\n\", i);\n",
        "  }\n",
        "}''')\n",
        "print(tokenizer.full_tokenize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc9KV1blc_k8"
      },
      "outputs": [],
      "source": [
        "def get_tokens(s):\n",
        "    tokenizer = Tokenizer(c_str=s)\n",
        "    res = tokenizer.full_tokenize()[0]\n",
        "    tag = [*res, \"\"][1:]\n",
        "    return (res, tag)\n",
        "def run_model(model, x):\n",
        "    model.train(False)\n",
        "    inputs = prepare_sequence(x, token_to_ix).to(device).view(1,-1)\n",
        "    token_probs = model(inputs)\n",
        "    return token_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpbV_de_qP7E"
      },
      "outputs": [],
      "source": [
        "def localise(model, x):\n",
        "    p = run_model(model, x).transpose(0,2).view(len(x), 178)\n",
        "    listCorrections = []\n",
        "    for t in range(len(x)):\n",
        "        xBtn = tokenlist[p[t].argmax()]\n",
        "        try:\n",
        "            if x[t+1]!=xBtn:\n",
        "                listCorrections.append([p[t].argmin().item(), x[t], x[t+1], xBtn, t, t+1, t+2])\n",
        "        except:\n",
        "            continue\n",
        "    listCorrections.sort(key=lambda x: x[0])\n",
        "    return listCorrections\n",
        "def feature_vectors(model, x):\n",
        "    count = [0]*178\n",
        "    for i in x:\n",
        "        count[token_to_ix.get(i)]+=1\n",
        "    loc = localise(model, x)\n",
        "    v1 = v2 = v3 = [0]*178\n",
        "    v1[token_to_ix.get(loc[0][1],0)]=loc[0][4]\n",
        "    v2[token_to_ix.get(loc[0][2],0)]=loc[0][5]\n",
        "    v3[token_to_ix.get(loc[0][3],0)]=loc[0][6]\n",
        "    return v1+v2+v3+count\n",
        "def probabalise(model, x):\n",
        "    p = run_model(model, x).transpose(0,2).view(len(x), 178)\n",
        "    #177,p(token)\n",
        "    #batchno\n",
        "    #len(string)-p(nexttoken)\n",
        "    print(p.size())\n",
        "    print(p[0].size())\n",
        "    listCorrections = []\n",
        "    y=[*x,0][1:]\n",
        "    for t, pt in enumerate(p):\n",
        "        # try:\n",
        "        listCorrections.append((x[t], y[t], pt[token_to_ix[y[t]]].item(), pt[pt.argmax()].item(), tokenlist[pt.argmax()]))\n",
        "        # except:\n",
        "        #     pass\n",
        "    return listCorrections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CCJdTrAqXjG"
      },
      "outputs": [],
      "source": [
        "s=\"\"\"\n",
        "#include <stdio.h>\n",
        "\n",
        "int ret(int x){\n",
        "  return --x;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  printf(\"Hello World!\\n\")\n",
        "  int x = ret(1);\n",
        "  return x;\n",
        "}\n",
        "\"\"\"\n",
        "\" \".join(Tokenizer(c_str=s).full_tokenize()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m1qERKusTdU"
      },
      "outputs": [],
      "source": [
        "probabalise(model, get_tokens(s)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ocEYbLFT1Ic"
      },
      "outputs": [],
      "source": [
        "localise(model, get_tokens(s)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxGMRd8yk-Ku"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "y_test = np.array([])\n",
        "y_preb_probs = np.array([])\n",
        "y_pred = np.array([])\n",
        "nb_classes = 178\n",
        "head_dataset = CDataset('final.pckl',200)\n",
        "data_loader = torch.utils.data.DataLoader(head_dataset, batch_size=1, num_workers=2)\n",
        "confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in tqdm(enumerate(data_loader), desc=\"data\", total=len(dataset)):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model(inputs)\n",
        "        y_test = np.append(y_test, inputs.cpu().detach().numpy())\n",
        "        y_preb_probs = np.append(y_preb_probs, outputs.cpu().detach().numpy().reshape)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_pred = np.append(y_pred, preds.view(-1).cpu().detach().numpy())\n",
        "        for t, p in tqdm(zip(classes.view(-1), preds.view(-1)), desc=\"tagging\", leave=False):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "gmap = {\"variable\":0, \"function\":1, \"keyword\": 2, \"alphabet\": 3, \"numbers\": 4, \"punctuation\": 5}\n",
        "gcm=np.zeros((6, 6))\n",
        "for ix, i in enumerate(confusion_matrix):\n",
        "    for jx, j in enumerate(i):\n",
        "        gcm[gmap[get_group(ix)]][gmap[get_group(jx)]]+=j\n",
        "class_names = [\"variable\", \"function\", \"keyword\", \"alphabet\", \"numbers\", \"punctuation\"]\n",
        "df_cm = pd.DataFrame(gcm, index=class_names, columns=class_names).astype(int)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.set(font_scale=1.3)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\",ax=ax,annot_kws={'size': 20})\n",
        "\n",
        "\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=20)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=20)\n",
        "plt.ylabel('True label',fontsize=20)\n",
        "plt.xlabel('Predicted label',fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8stymp7fpHMy"
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsGJuQnHyCaz"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi-m_r3zyOTf"
      },
      "outputs": [],
      "source": [
        "confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zYOiyVBVHxL"
      },
      "outputs": [],
      "source": [
        "gcm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrQkZMeHSKp5"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO6wrkZzSNJi"
      },
      "outputs": [],
      "source": [
        "y_preb_probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNFsb_g7TKHc"
      },
      "outputs": [],
      "source": [
        "y_preb_probs=np.array([x/sum(x) for x in y_preb_probs.reshape((-1,178))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IhDlBbKVUS2"
      },
      "outputs": [],
      "source": [
        "sum(y_preb_probs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK5u3ZAhSaBg"
      },
      "outputs": [],
      "source": [
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ch7k1EW18jT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import log_loss\n",
        "print(roc_auc_score(y_test, y_preb_probs, average=\"weighted\", multi_class=\"ovr\"))\n",
        "print(cohen_kappa_score(y_test, y_pred))\n",
        "print(matthews_corrcoef(y_test, y_pred))\n",
        "#print(log_loss(y_test, y_preb_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_MPfZenyRne"
      },
      "outputs": [],
      "source": [
        "confusion_matrix\n",
        "total = sum(sum(confusion_matrix))\n",
        "correct = sum(np.diag(confusion_matrix))\n",
        "accuracy = correct/total\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKO6WheK0DB9"
      },
      "outputs": [],
      "source": [
        "# Calculate TP, TN, FP, FN for each class\n",
        "TP = np.diag(confusion_matrix)\n",
        "FP = np.sum(confusion_matrix, axis=0) - TP\n",
        "FN = np.sum(confusion_matrix, axis=1) - TP\n",
        "\n",
        "# Calculate metrics for each class\n",
        "precision = np.zeros(nb_classes)\n",
        "recall = np.zeros(nb_classes)\n",
        "f1_score = np.zeros(nb_classes)\n",
        "\n",
        "for i in range(nb_classes):\n",
        "    if TP[i] == 0 and (FP[i] == 0 or FN[i] == 0):\n",
        "        precision[i] = 0\n",
        "        recall[i] = 0\n",
        "    else:\n",
        "        precision[i] = TP[i] / (TP[i] + FP[i])\n",
        "        recall[i] = TP[i] / (TP[i] + FN[i])\n",
        "    f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0\n",
        "\n",
        "# Print metrics in percentage\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "print(\"Precision: {:.2%}\".format(np.mean(precision)))\n",
        "print(\"Recall: {:.2%}\".format(np.mean(recall)))\n",
        "print(\"F1-score: {:.2%}\".format(np.mean(f1_score)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW1KOWsG7daC"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "slider = widgets.Textarea()\n",
        "display(slider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmb2htvy-Deh"
      },
      "outputs": [],
      "source": [
        "print(slider.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh8DgLOc7-GX"
      },
      "outputs": [],
      "source": [
        "probabalise(model, get_tokens(slider.value)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE0QkxFH34EX"
      },
      "outputs": [],
      "source": [
        "localise(model, get_tokens(slider.value)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccpZSATGW6Cn"
      },
      "outputs": [],
      "source": [
        "with open(\"vectors.pckl\", 'rb') as f:\n",
        "    vectors=pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nybssRpHXZfJ"
      },
      "outputs": [],
      "source": [
        "vectors=pd.DataFrame(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGGKKcC7X-Y6"
      },
      "outputs": [],
      "source": [
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiA9MVYHj7Gn"
      },
      "outputs": [],
      "source": [
        "X = vectors.head(10000).apply(lambda x: x.vec1+x.vec2+x.vec3+x[\"count\"], axis=1)\n",
        "y = vectors.head(10000).op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0se-3JSZhn1"
      },
      "outputs": [],
      "source": [
        "clf = svm.SVC()\n",
        "clf.fit(list(X), y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wgNNUcgtK4-"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(list(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWVpQuVeyXol"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "accuracy = metrics.accuracy_score(y,y_pred)\n",
        "accuracy\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LsatD_xcovU"
      },
      "outputs": [],
      "source": [
        "with open(\"svc.pckl\", 'rb') as f:\n",
        "    clf = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V78XnKrYnBw"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyd9ZHNSYpG1"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Izn6qpYXfN"
      },
      "outputs": [],
      "source": [
        "with open(\"svc.pckl\", 'wb') as f:\n",
        "    pickle.dump(clf, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlCStqtAlMr6"
      },
      "outputs": [],
      "source": [
        "s=\"\"\"\n",
        "#include <stdio.h>\n",
        "\n",
        "int ret(int x){\n",
        "  return --x;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  printf(\"Hello World!\\n\")\n",
        "  int x = ret(1);\n",
        "  return x;\n",
        "}\n",
        "\"\"\"\n",
        "x=Tokenizer(c_str=s).full_tokenize()[0]\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSVHBDTBloL_"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/models/deep_model_20230411_121014_fold_4_epoch_0\", map_location=device))\n",
        "l= localise(model, x)[0]\n",
        "fv= [feature_vectors(model, x)]\n",
        "l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcxJElj0gQrX"
      },
      "outputs": [],
      "source": [
        "print(fv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjPESweyeSEm"
      },
      "outputs": [],
      "source": [
        "clf.predict(fv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo2dPmQ3gZR5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
